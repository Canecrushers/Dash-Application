{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kunal\\Desktop\\WORK\\Datathon\\Phase02-DataDelivery\\masks\\mask-x7680-y10240.png\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-988758ebd142>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mmask_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mask_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTILE_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTILE_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     \u001b[0mmasp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'P'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcropbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcropbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m     \u001b[0mmask_pixels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpixels_in_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7680\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10240\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[0mtci_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_timeseries_image_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTILE_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTILE_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TCI'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WORK\\Datathon\\Github\\Dash-Application\\CCfunctions.py\u001b[0m in \u001b[0;36mopen_image\u001b[1;34m(path, mode, cropbox, verbose)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:\\\\Users\\\\kunal\\\\Desktop\\\\WORK\\\\Datathon\\\\Phase02-DataDelivery\\\\'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf\"masks\\\\mask-x{tile_x}-y{tile_y}.png\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "#Importing Packages\n",
    "from CCfunctions import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "#from tqdm import tqdm, trange, tqdm_notebook\n",
    "from time import sleep\n",
    "from math import sqrt\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "scaler = StandardScaler()\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import boto3\n",
    "import io\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from pandas import DataFrame\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import plotly\n",
    "#from pyspark.ml.clustering import KMeans\n",
    "#from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "#from pyspark.ml.feature import VectorAssembler\n",
    "#from pyspark.sql import SQLContext\n",
    "#from pyspark import SparkContext\n",
    "#from pyspark import SparkConf\n",
    "#from pyspark.context import SparkContext\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.externals import joblib\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "\n",
    "# \\\\Users\\\\kunal\\\\Desktop\\\\WORK\\\\Datathon\\\\Phase02-DataDelivery\n",
    "\n",
    "# Importing \n",
    "sugarcanetiles = 'C:\\\\Users\\\\kunal\\\\Desktop\\\\WORK\\\\Datathon\\\\Phase02-DataDelivery\\\\sugarcanetiles'\n",
    "output_image_folder = 'C:\\\\Users\\\\kunal\\\\Desktop\\\\WORK\\\\Datathon\\\\Images\\\\'\n",
    "output_parquet_folder = 'C:\\\\Users\\\\kunal\\\\Desktop\\\\WORK\\\\Datathon\\\\Parquets\\\\'\n",
    "\n",
    "onlyfiles = [f for f in listdir(sugarcanetiles) if isfile(join(sugarcanetiles, f))]\n",
    "\n",
    "# load Model\n",
    "\n",
    "kms_model_file = '15ImageModel_model.sav'\n",
    "#joblib.dump(model, kms_model_file)\n",
    "\n",
    "Kmean = joblib.load(kms_model_file)\n",
    "\n",
    "# where Unclean means original image (Tile x Time), where an image is a passing of the satelight over a tile of ground\n",
    "all_Unclean_Images_DF = pd.DataFrame()\n",
    "all_Unclean_Images_DF['Unclean'] = onlyfiles\n",
    "all_Unclean_Images_DF['Unclean'].str.split('-',expand=True)\n",
    "all_Unclean_Images_DF['X Tile'] =  all_Unclean_Images_DF['Unclean'].str.split('-',expand=True)[0]\n",
    "all_Unclean_Images_DF['Y Tile'] =  all_Unclean_Images_DF['Unclean'].str.split('-',expand=True)[1]\n",
    "all_Unclean_Images_DF['TileTuple'] = all_Unclean_Images_DF['X Tile'].astype(str) + \" \"+ all_Unclean_Images_DF['Y Tile'].astype(str)\n",
    "\n",
    "# create SelectedDF to contain a set of tiles to be processed  : a tile is a square of ground\n",
    "SelectedTiles = ['7680 10240']\n",
    "SelecteDF  = all_Unclean_Images_DF[all_Unclean_Images_DF['TileTuple'].isin(SelectedTiles)]\n",
    "SelecteDF = SelecteDF[['X Tile','Y Tile','TileTuple']].drop_duplicates()\n",
    "\n",
    "# create the column headings for the resulting dataframe to be saved as parquet\n",
    "columns = [\"tile_x\",\"tile_y\", \"x\",\"y\", \"date\", \"mask\", \"red\",\"green\",\"blue\"]\n",
    "columns.extend([f'B{b:02d}' for b in range(1,13)])\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# columns to be used for the data model\n",
    "vegi1InputCols = ['Scaled_NDVI', 'Scaled_LCI', 'Scaled_LAI', 'Scaled_GNDVI', 'Scaled_SCI']\n",
    "\n",
    "for index,row in SelecteDF.iterrows():\n",
    "    TILE_X = row['X Tile']\n",
    "    TILE_Y = row['Y Tile']\n",
    "\n",
    "    (start_x, start_y) = (0, 0)\n",
    "    (size_x, size_y) = (512, 512)\n",
    "    cropbox = (start_x, start_y, start_x + size_x, start_y + size_y)\n",
    "    \n",
    "    mask_path = get_mask_path(TILE_X, TILE_Y)\n",
    "    masp = open_image(mask_path, mode = 'P', cropbox = cropbox)\n",
    "    mask_pixels = pixels_in_mask(7680, 10240)\n",
    "    tci_list = get_timeseries_image_paths(TILE_X, TILE_Y, 'TCI')\n",
    "\n",
    "    b_name_list = [f'B{b:02d}' for b in range(1,13)]\n",
    "    b_path_lol = [get_timeseries_image_paths(TILE_X, TILE_Y, b) for b in b_name_list]\n",
    "\n",
    "    ModelDF = pd.DataFrame()\n",
    "    assert len(tci_list) == len(b_path_lol[0])\n",
    "    (tile_x, tile_y) = (TILE_X, TILE_Y)\n",
    "    \n",
    "    \n",
    "    for day_no in range(0,1): # len(tci_list)):\n",
    "        tci_path = tci_list[day_no]\n",
    "        date = last_date_in_path(tci_path)\n",
    "        b_path_list = [b_path_list[day_no] for b_path_list in b_path_lol]\n",
    "\n",
    "        tci_img = open_image(tci_path, cropbox = cropbox, verbose=False)\n",
    "        b_img_list = [open_image(b_path, cropbox = cropbox, verbose=False) for b_path in b_path_list]\n",
    "\n",
    "        data = read_img_pixel_values(tile_x, tile_y, date, masp, tci_img, *b_img_list)\n",
    "\n",
    "        df = pd.DataFrame(columns=columns, data=data)\n",
    "\n",
    "        # Calculate Indices:\n",
    "        # https://support.micasense.com/hc/en-us/articles/227837307-An-overview-of-the-available-layers-and-indices-in-Atlas\n",
    "        # https://support.micasense.com/hc/en-us/articles/226531127-Creating-agricultural-indices-NDVI-NDRE-from-an-Atlas-GeoTIFF-in-QGIS-\n",
    "        # https://earth.esa.int/web/sentinel/technical-guides/sentinel-2-msi/level-2a/algorithm\n",
    "        # https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/indexdb/\n",
    "        # NDVI ref: https://medium.com/analytics-vidhya/satellite-imagery-analysis-with-python-3f8ccf8a7c32\n",
    "\n",
    "        # NDVI - Normalised Difference Vegetation Index (NDVI)\n",
    "        #  = (NIR - RED) / (NIR + RED)\n",
    "        df['NDVI'] = df.apply(lambda df : int(10000 * (float(df.B08) - float(df.B04)) / (df.B08 + df.B04)) , axis=1)\n",
    "\n",
    "        # GNDVI - Green Normalized Difference NDVI\n",
    "        #  = (NIR - GREEN)/(NIR + GREEN)\n",
    "        df['GNDVI'] = df.apply(lambda df : int(10000 * (float(df.B08) - float(df.B03)) / (df.B08 + df.B03)) , axis=1)\n",
    "\n",
    "        # RDVI - Normalised Difference Vegetation Index (NDVI)\n",
    "        #  = 2*(NIR - RED) / sqrt(NIR + RED)\n",
    "        df['RDVI'] = df.apply(lambda df : int(10000 * 2 * (float(df.B08) - float(df.B04)) / sqrt(df.B08 + df.B04)) , axis=1)\n",
    "\n",
    "        # RBNDVI - Red Blue Normalised Difference Vegetation Index (NDVI)\n",
    "        #  = (NIR - RED -BLUE) / (NIR + RED + BLUE)\n",
    "        df['RBNDVI'] = df.apply(lambda df : int(10000 * (2*float(df.B08) - float(df.B04) - float(df.B02)) / (2*df.B08 + df.B04 + df.B02)) , axis=1)\n",
    "\n",
    "        # LCI - Leaf Chlorophyll Index\n",
    "        #  = (NIR - REDE)/(NIR + REDE)\n",
    "        df['LCI'] = df.apply(lambda df : int(100 * (float(df.B08) - float(df.B05)) / (df.B08 + df.B05)) , axis=1)\n",
    "\n",
    "        # LAI - Leaf Area Index\n",
    "        #  = (REDE - RED)/(REDE + RED)\n",
    "        # https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3231680/\n",
    "        df['LAI'] = df.apply(lambda df : int(10000 * (float(df.B05) - float(df.B04)) / (df.B05 + df.B04)) , axis=1)\n",
    "\n",
    "        # SCI - Soil Composition Index\n",
    "        #  = (SWIR - NIR)/(SWIR + NIR)\n",
    "        df['SCI'] = df.apply(lambda df : int(10000 * (float(df.B11) - float(df.B08)) / (df.B11 + df.B08)) , axis=1)\n",
    "\n",
    "        # SCI - Soil Composition Index\n",
    "        #  = (SWIR - NIR)/(SWIR + NIR)\n",
    "        df['SCI'] = df.apply(lambda df : int(10000 * (float(df.B11) - float(df.B08)) / (df.B11 + df.B08)) , axis=1)\n",
    "\n",
    "        # NDMI - Normalized Difference Moisture Index\n",
    "        #  = (NIR - SWIR)/(SWIR + NIR)\n",
    "        df['NDMI'] = df.apply(lambda df : int(10000 * (float(df.B08) - float(df.B11)) / (df.B11 + df.B08)) , axis=1)\n",
    "\n",
    "        # GLI - Green Leaf Index\n",
    "        #  = (GREEN - BLUE)/(GREEN + BLUE)\n",
    "        df['GLI'] = df.apply(lambda df : int(10000 * (float(df.B05) - float(df.B04)) / (df.B05 + df.B04)) , axis=1)\n",
    "\n",
    "        # ModelDF = pd.concat([ModelDF,df]).reset_index(drop=True)\n",
    "        df = df[(df['red']<225)&(df['green']<225)&(df['blue']<225)].reset_index(drop=True)\n",
    "\n",
    "        df['Scaled_NIR'] = min_max_scaler.fit_transform(df[['B08']])\n",
    "        df['Scaled_RED'] = min_max_scaler.fit_transform(df[['B04']])\n",
    "        df['Scaled_GRN'] = min_max_scaler.fit_transform(df[['B03']])\n",
    "        df['Scaled_NDVI'] = min_max_scaler.fit_transform(df[['NDVI']])\n",
    "        df['Scaled_LCI'] = min_max_scaler.fit_transform(df[['LCI']])\n",
    "        df['Scaled_LAI'] = min_max_scaler.fit_transform(df[['LAI']])\n",
    "        df['Scaled_SCI'] = min_max_scaler.fit_transform(df[['SCI']])\n",
    "        df['Scaled_GNDVI'] = min_max_scaler.fit_transform(df[['GNDVI']])\n",
    "\n",
    "        df['PixelTuple'] = df['x'].astype(str) + \" \" + df['y'].astype(str)\n",
    "        dfMask = df[df['PixelTuple'].isin(mask_pixels)].reset_index(drop=True)\n",
    "        dfMask['prediction']= Kmean.predict(dfMask[vegi1InputCols])\n",
    "        tciFilePath = sugarcanetiles + str(TILE_X)+\"-\"+ str(TILE_Y) + \"-TCI-\" + date+'.png'\n",
    "        #tciFilePath = sugarcanetiles + '\\\\7680-10240-TCI-2017-06-20.png'\n",
    "        tci = open_image(tciFilePath, mode='RGB')\n",
    "        numberOfClusters = 2\n",
    "        vegiClustered_df = dfMask[['x','y','prediction']]\n",
    "\n",
    "        colours = [\n",
    "            np.array([0x10,0xAD,0x00], dtype='uint8'),  # limish green\n",
    "            np.array([0x00,0x26,0xA4], dtype='uint8'),  # blueish\n",
    "            np.array([0xFF,0x00,0x00], dtype='uint8'),  # red\n",
    "            np.array([0xFF,0xC6,0x00], dtype='uint8'),  # yellow\n",
    "            np.array([0x00, 0x00, 0x00],dtype='uint8')\n",
    "            ]\n",
    "\n",
    "        imgnp = overlayPredictionImage(vegiClustered_df, tci, colours)\n",
    "        # imgnp.shape\n",
    "\n",
    "        clusterImg = Image.fromarray(np.hstack((imgnp, np.array(tci))))\n",
    "\n",
    "        path = 'C:\\\\Users\\\\kunal\\\\Desktop\\\\WORK\\\\Datathon\\\\Github\\\\Dash-Application\\\\Images\\\\'\n",
    "        path += f\"comp_image_{TILE_X}_{TILE_Y}_{date}.png\"\n",
    "        print(type(clusterImg))\n",
    "        clusterImg.save(path)\n",
    "        path = output_parquet_folder\n",
    "        path += f\"image_values_{TILE_X}_{TILE_Y}_{date}.snappy.parquet\"\n",
    "        vegiClustered_df.to_parquet(path)\n",
    "\n",
    "\n",
    "    p = Pool(50)\n",
    "    p.map( processTile, [row for index, row in SelecteDF.iterrows()[0:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tile_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e7f8d1d3c30f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtile_x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tile_x' is not defined"
     ]
    }
   ],
   "source": [
    "tile_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
